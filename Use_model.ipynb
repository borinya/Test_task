{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchaudio import transforms\n",
    "import torch.nn as nn\n",
    "import librosa # for mel-spectrogram estimation\n",
    "import soundfile # for opening .flac audio\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_audio, framerate = soundfile.read('explore_data/'+ 'audio_samples/' + '20-205-0000.flac')\n",
    "noisy_audio, framerate = soundfile.read('explore_data/'+ 'audio_samples/' + '20-205-0000_noisy.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_audio.shape\n",
    "noisy_audio.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(clean_audio) / float(framerate)) # length in seconds\n",
    "print(len(noisy_audio) / float(framerate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized log-mel-spectrogram of clean and noisy audios\n",
    "clean_mel = 1 + np.log(1.e-12 + librosa.feature.melspectrogram(clean_audio, sr=16000, n_fft=1024, hop_length=256, fmin=20, fmax=8000, n_mels=80)).T / 10.\n",
    "noisy_mel = 1 + np.log(1.e-12 + librosa.feature.melspectrogram(noisy_audio, sr=16000, n_fft=1024, hop_length=256, fmin=20, fmax=8000, n_mels=80)).T / 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_mel.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(clean_mel.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(noisy_mel.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aud = torch.from_numpy((noisy_mel).T)\n",
    "aud = torch.from_numpy((clean_mel).T)\n",
    "num_rows, sig_len = aud.shape # возможно тут местами поменять строки\n",
    "pad_begin_len = random.randint(0, 1400 - sig_len) # 1400 длина наша максимальная, растягиваю все данные до 1400 измерений\n",
    "pad_end_len = 1400 - sig_len - pad_begin_len\n",
    "\n",
    "# Pad with 0s\n",
    "pad_begin = torch.zeros((num_rows, pad_begin_len))\n",
    "pad_end = torch.zeros((num_rows, pad_end_len))\n",
    "\n",
    "aud = torch.cat((pad_begin, aud, pad_end), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = aud\n",
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
    "inputs = (inputs - inputs_m) / inputs_s\n",
    "inputs = torch.repeat_interleave(inputs,3,dim = 0)\n",
    "inputs = inputs.view(1,3,80,1400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = torch.load(r'C:\\Users\\boris\\Documents\\notebok\\госзнак\\best_model.pth')\n",
    "best_model.eval()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "best_model = best_model.to(device)\n",
    "inputs = inputs.to(device).type(torch.cuda.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "type(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = best_model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'explore_data/'+ 'audio_samples/' + '20-205-0000.flac'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchaudio import transforms\n",
    "import torch.nn as nn\n",
    "import librosa # for mel-spectrogram estimation\n",
    "import soundfile # for opening .flac audio\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_model(path_to_flac, path_to_model):\n",
    "    audio, framerate = soundfile.read(path_to_flac)\n",
    "    mel = 1 + np.log(1.e-12 + librosa.feature.melspectrogram(audio, sr=16000, n_fft=1024, hop_length=256, fmin=20, fmax=8000, n_mels=80)).T / 10.\n",
    "    audio_input = torch.from_numpy((mel).T)\n",
    "    num_rows, sig_len = audio_input.shape\n",
    "    pad_begin_len = random.randint(0, 1400 - sig_len) # 1400 длина наша максимальная, растягиваю все данные до 1400 измерений\n",
    "    pad_end_len = 1400 - sig_len - pad_begin_len\n",
    "    pad_begin = torch.zeros((num_rows, pad_begin_len))\n",
    "    pad_end = torch.zeros((num_rows, pad_end_len))\n",
    "    audio_input = torch.cat((pad_begin, audio_input, pad_end), 1)\n",
    "    inputs_m, inputs_s = audio_input.mean(), audio_input.std()\n",
    "    audio_input = (audio_input - inputs_m) / inputs_s\n",
    "    audio_input = torch.repeat_interleave(audio_input,3,dim = 0)\n",
    "    audio_input = audio_input.view(1,3,80,1400)\n",
    "    best_model = torch.load(path_to_model)\n",
    "    best_model.eval()\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    best_model = best_model.to(device)\n",
    "    audio_input = audio_input.to(device).type(torch.cuda.FloatTensor)\n",
    "    out = best_model(audio_input)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.9944, -5.2343]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_model('explore_data/'+ 'audio_samples/' + '20-205-0000.flac',r'C:\\Users\\boris\\Documents\\notebok\\госзнак\\best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
